{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b35445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leopu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\leopu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\leopu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leopu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b434b",
   "metadata": {},
   "source": [
    "### Choosing a document unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d2dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Rings for the Elven-kings under the sky,\n",
      "               Seven for the Dwarf-lords in their halls of stone,\n",
      "            Nine for Mortal Men doomed to die,\n",
      "              One for the Dark Lord on his dark throne\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "               One Ring to rule them all, One Ring to find them,\n",
      "               One Ring to bring them all and in the darkness bind them\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the lotr.txt file\n",
    "file_path = Path(\"../../lotr.txt\")\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "print(text[:468])\n",
    "\n",
    "chapters = text.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7c48f",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b57223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Three', 'Rings', 'for', 'the', 'Elven-kings', 'under', 'the', 'sky', ',', 'Seven']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8176d71",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b85aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', \"she'd\", 'hers', 'did', 'wasn', 'then', 'after', 'hasn', 'up', \"they're\"]\n",
      "['Three', 'Rings', 'Elven-kings', 'sky', ',', 'Seven', 'Dwarf-lords', 'halls', 'stone', ',']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Stop Word Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(list(stop_words)[:10])\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537072d",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56cf0c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'rings', 'elven-kings', 'sky', 'seven', 'dwarf-lords', 'halls', 'stone', 'nine', 'mortal']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Normalization\n",
    "def normalize_text(tokens):\n",
    "    # Convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "normalized_tokens = normalize_text(filtered_tokens)\n",
    "print(normalized_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e5f98",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e659ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['three', 'ring', 'elven-kings', 'sky', 'seven', 'dwarf-lords', 'hall', 'stone', 'nine', 'mortal']\n",
      "Stemmed Tokens: ['three', 'ring', 'elven-k', 'sky', 'seven', 'dwarf-lord', 'hall', 'stone', 'nine', 'mortal']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Stemming and Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in normalized_tokens]\n",
    "stemmed_tokens = [stemmer.stem(word) for word in normalized_tokens]\n",
    "\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens[:10])\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627ae4d7",
   "metadata": {},
   "source": [
    "### Creating an inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019efdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create a Positional Index\n",
    "positional_index = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for doc_id, chapter in enumerate(chapters):\n",
    "    tokens = word_tokenize(chapter)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    normalized_tokens = normalize_text(filtered_tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in normalized_tokens]\n",
    "\n",
    "    for position, token in enumerate(lemmatized_tokens):\n",
    "        positional_index[token][doc_id].append(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364b987",
   "metadata": {},
   "source": [
    "The positional_index is a nested dictionary where the first key is the term, the second key is the document ID, and the value is a list of positions where the term appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22185969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {14: [136],\n",
       "             16: [0],\n",
       "             341: [3],\n",
       "             342: [266, 280],\n",
       "             345: [32],\n",
       "             484: [337],\n",
       "             574: [343],\n",
       "             575: [471],\n",
       "             780: [515],\n",
       "             794: [52],\n",
       "             801: [1070],\n",
       "             833: [480]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_index[\"fellowship\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47659de8",
   "metadata": {},
   "source": [
    "### Posting List Node and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71cd515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define Positional Posting List Node and Positional Posting List classes\n",
    "class PositionalPostingListNode:\n",
    "    def __init__(self, doc_id, positions):\n",
    "        self.doc_id = doc_id\n",
    "        self.positions = positions\n",
    "        self.next = None\n",
    "        self.skip = None\n",
    "\n",
    "class PositionalPostingList:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "\n",
    "    def add_document(self, doc_id, positions):\n",
    "        new_node = PositionalPostingListNode(doc_id, positions)\n",
    "        if not self.head:\n",
    "            self.head = new_node\n",
    "        else:\n",
    "            current = self.head\n",
    "            while current.next:\n",
    "                current = current.next\n",
    "            current.next = new_node\n",
    "\n",
    "    def add_skip_pointers(self, skip_length):\n",
    "        current = self.head\n",
    "        while current:\n",
    "            skip_node = current\n",
    "            for _ in range(skip_length):\n",
    "                if skip_node:\n",
    "                    skip_node = skip_node.next\n",
    "                else:\n",
    "                    break\n",
    "            current.skip = skip_node\n",
    "            current = current.next\n",
    "\n",
    "    def search(self, doc_id):\n",
    "        current = self.head\n",
    "        while current:\n",
    "            if current.doc_id == doc_id:\n",
    "                return True\n",
    "            if current.skip and current.skip.doc_id <= doc_id:\n",
    "                current = current.skip\n",
    "            else:\n",
    "                current = current.next\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d94c4",
   "metadata": {},
   "source": [
    "### Skip pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd121a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Create Positional Posting Lists with Skip Pointers\n",
    "positional_posting_lists = {}\n",
    "\n",
    "for term, postings in positional_index.items():\n",
    "    positional_posting_list = PositionalPostingList()\n",
    "    for doc_id, positions in postings.items():\n",
    "        positional_posting_list.add_document(doc_id, positions)\n",
    "    positional_posting_list.add_skip_pointers(3)\n",
    "    positional_posting_lists[term] = positional_posting_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db94d07",
   "metadata": {},
   "source": [
    "From the previous notebook, we can see that the term \"fellowship\" is present in these documents: [14, 16, 341, 342, 345, 484, 574, 575, 780, 794, 801, 833]\n",
    "\n",
    "If you search for any of these documents, it should return \"True\" if you search for \"fellowship\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa38437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term 'fellowship' found in document 14: True\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Search for a term in a document\n",
    "term_to_search = \"fellowship\"\n",
    "doc_id_to_search = 14\n",
    "\n",
    "if term_to_search in positional_posting_lists:\n",
    "    result = positional_posting_lists[term_to_search].search(doc_id_to_search)\n",
    "    print(f\"Term '{term_to_search}' found in document {doc_id_to_search}: {result}\")\n",
    "else:\n",
    "    print(f\"Term '{term_to_search}' not found in any document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76fdea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
