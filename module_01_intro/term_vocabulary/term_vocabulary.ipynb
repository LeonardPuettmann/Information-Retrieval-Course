{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4b35445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leopu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\leopu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\leopu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leopu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b434b",
   "metadata": {},
   "source": [
    "### Choosing a document unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7d2dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Rings for the Elven-kings under the sky,\n",
      "               Seven for the Dwarf-lords in their halls of stone,\n",
      "            Nine for Mortal Men doomed to die,\n",
      "              One for the Dark Lord on his dark throne\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "               One Ring to rule them all, One Ring to find them,\n",
      "               One Ring to bring them all and in the darkness bind them\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the lotr.txt file\n",
    "file_path = Path(\"../../lotr.txt\")\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "print(text[:468])\n",
    "\n",
    "chapters = text.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7c48f",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b57223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Three', 'Rings', 'for', 'the', 'Elven-kings', 'under', 'the', 'sky', ',', 'Seven']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8176d71",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b85aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['theirs', \"she'll\", 'below', 'so', 'down', \"we'll\", 'ain', \"i'd\", 'have', \"wasn't\"]\n",
      "['Three', 'Rings', 'Elven-kings', 'sky', ',', 'Seven', 'Dwarf-lords', 'halls', 'stone', ',']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Stop Word Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(list(stop_words)[:10])\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537072d",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56cf0c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three', 'rings', 'elven-kings', 'sky', 'seven', 'dwarf-lords', 'halls', 'stone', 'nine', 'mortal']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Normalization\n",
    "def normalize_text(tokens):\n",
    "    # Convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "normalized_tokens = normalize_text(filtered_tokens)\n",
    "print(normalized_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e5f98",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e659ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['three', 'ring', 'elven-kings', 'sky', 'seven', 'dwarf-lords', 'hall', 'stone', 'nine', 'mortal']\n",
      "Stemmed Tokens: ['three', 'ring', 'elven-k', 'sky', 'seven', 'dwarf-lord', 'hall', 'stone', 'nine', 'mortal']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Stemming and Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in normalized_tokens]\n",
    "stemmed_tokens = [stemmer.stem(word) for word in normalized_tokens]\n",
    "\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens[:10])\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627ae4d7",
   "metadata": {},
   "source": [
    "### Creating an inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "019efdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create an Inverted Index\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "for doc_id, chapter in enumerate(chapters):\n",
    "    tokens = word_tokenize(chapter)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    normalized_tokens = normalize_text(filtered_tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in normalized_tokens]\n",
    "\n",
    "    for position, token in enumerate(lemmatized_tokens):\n",
    "        inverted_index[token].append((doc_id, position))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47659de8",
   "metadata": {},
   "source": [
    "### Posting List Node and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71cd515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define Posting List Node and Posting List classes\n",
    "class PostingListNode:\n",
    "    def __init__(self, doc_id, position):\n",
    "        self.doc_id = doc_id\n",
    "        self.position = position\n",
    "        self.next = None\n",
    "        self.skip = None\n",
    "\n",
    "class PostingList:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "\n",
    "    def add_document(self, doc_id, position):\n",
    "        new_node = PostingListNode(doc_id, position)\n",
    "        if not self.head:\n",
    "            self.head = new_node\n",
    "        else:\n",
    "            current = self.head\n",
    "            while current.next:\n",
    "                current = current.next\n",
    "            current.next = new_node\n",
    "\n",
    "    def add_skip_pointers(self, skip_length):\n",
    "        current = self.head\n",
    "        while current:\n",
    "            skip_node = current\n",
    "            for _ in range(skip_length):\n",
    "                if skip_node:\n",
    "                    skip_node = skip_node.next\n",
    "                else:\n",
    "                    break\n",
    "            current.skip = skip_node\n",
    "            current = current.next\n",
    "\n",
    "    def search(self, doc_id):\n",
    "        current = self.head\n",
    "        while current:\n",
    "            if current.doc_id == doc_id:\n",
    "                return True\n",
    "            if current.skip and current.skip.doc_id <= doc_id:\n",
    "                current = current.skip\n",
    "            else:\n",
    "                current = current.next\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d94c4",
   "metadata": {},
   "source": [
    "### Skip pointers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450964d",
   "metadata": {},
   "source": [
    "Skip pointers are used to optimize the search process in a posting list, which is a fundamental data structure in information retrieval systems. Here are some reasons why skip pointers are beneficial:\n",
    "\n",
    "- Faster Search: Skip pointers allow you to skip over sections of the posting list during a search, reducing the number of comparisons needed to find a specific document ID. This can significantly speed up the search process, especially for large posting lists.\n",
    "\n",
    "- Efficient Traversal: By using skip pointers, you can traverse the posting list more efficiently. Instead of checking every single node, you can jump ahead by a certain number of nodes, which reduces the time complexity of the search operation.\n",
    "\n",
    "- Balanced Performance: Skip pointers provide a balanced approach to improving search performance without significantly increasing the complexity of the data structure. They offer a good trade-off between the additional storage required for the skip pointers and the performance gains achieved during search operations.\n",
    "\n",
    "- Scalability: As the size of the posting list grows, the benefits of using skip pointers become more pronounced. Skip pointers help maintain efficient search performance even as the dataset scales up.\n",
    "\n",
    "- Improved Query Processing: In information retrieval systems, query processing often involves intersecting or merging multiple posting lists. Skip pointers can make these operations more efficient by reducing the number of nodes that need to be examined.\n",
    "\n",
    "### Example Scenario:\n",
    "Imagine you have a posting list with document IDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. Without skip pointers, searching for a document ID would require checking each node sequentially until you find the desired ID.\n",
    "\n",
    "With skip pointers added at intervals of 3, the posting list might look like this:\n",
    "\n",
    "Node 1: skip pointer to Node 4\n",
    "Node 2: skip pointer to Node 5\n",
    "Node 3: skip pointer to Node 6\n",
    "Node 4: skip pointer to Node 7\n",
    "Node 5: skip pointer to Node 8\n",
    "Node 6: skip pointer to Node 9\n",
    "Node 7: skip pointer to Node 10\n",
    "Node 8: skip pointer to Node 10\n",
    "Node 9: skip pointer to Node 10\n",
    "Node 10: no skip pointer\n",
    "When searching for document ID 7, you can start at Node 1 and use the skip pointer to jump to Node 4. From Node 4, you can jump to Node 7 directly using the skip pointer, significantly reducing the number of nodes you need to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fd121a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Create Posting Lists with Skip Pointers\n",
    "posting_lists = {}\n",
    "\n",
    "for term, postings in inverted_index.items():\n",
    "    posting_list = PostingList()\n",
    "    for doc_id, position in postings:\n",
    "        posting_list.add_document(doc_id, position)\n",
    "    posting_list.add_skip_pointers(3)\n",
    "    posting_lists[term] = posting_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db94d07",
   "metadata": {},
   "source": [
    "From the previous notebook, we can see that the term \"fellowship\" is present in these documents: [14, 16, 341, 342, 345, 484, 574, 575, 780, 794, 801, 833]\n",
    "\n",
    "If you search for any of these documents, it should return \"True\" if you search for \"fellowship\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa38437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term 'fellowship' found in document 14: True\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Search for a term in a document\n",
    "term_to_search = \"fellowship\"\n",
    "doc_id_to_search = 14\n",
    "\n",
    "if term_to_search in posting_lists:\n",
    "    result = posting_lists[term_to_search].search(doc_id_to_search)\n",
    "    print(f\"Term '{term_to_search}' found in document {doc_id_to_search}: {result}\")\n",
    "else:\n",
    "    print(f\"Term '{term_to_search}' not found in any document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76fdea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
